<!doctype html><html lang=ko dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>코세라) 신경망 및 딥러닝 - 신경망으로서의 로지스틱 회귀 | Jaejin's blog</title>
<meta name=keywords content="neural network,구글 머신러닝 부트캠프"><meta name=description content="1. Binary Classification



y가 0, 1일 때 사용하는 분류입니다.
로지스틱 회귀(Logistic regression)는 이진 분류를 위한 알고리즘 중 하나입니다.

2. Notation

"><meta name=author content="Jaejin Jang"><link rel=canonical href=https://jaejin0me.github.io/post/20240713/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://jaejin0me.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://jaejin0me.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://jaejin0me.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://jaejin0me.github.io/apple-touch-icon.png><link rel=mask-icon href=https://jaejin0me.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=ko href=https://jaejin0me.github.io/post/20240713/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-F8504X956T"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-F8504X956T")}</script><meta property="og:title" content="코세라) 신경망 및 딥러닝 - 신경망으로서의 로지스틱 회귀"><meta property="og:description" content="1. Binary Classification



y가 0, 1일 때 사용하는 분류입니다.
로지스틱 회귀(Logistic regression)는 이진 분류를 위한 알고리즘 중 하나입니다.

2. Notation

"><meta property="og:type" content="article"><meta property="og:url" content="https://jaejin0me.github.io/post/20240713/"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-07-14T17:05:02+09:00"><meta property="article:modified_time" content="2024-07-14T17:05:02+09:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="코세라) 신경망 및 딥러닝 - 신경망으로서의 로지스틱 회귀"><meta name=twitter:description content="1. Binary Classification



y가 0, 1일 때 사용하는 분류입니다.
로지스틱 회귀(Logistic regression)는 이진 분류를 위한 알고리즘 중 하나입니다.

2. Notation

"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://jaejin0me.github.io/post/"},{"@type":"ListItem","position":2,"name":"코세라) 신경망 및 딥러닝 - 신경망으로서의 로지스틱 회귀","item":"https://jaejin0me.github.io/post/20240713/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"코세라) 신경망 및 딥러닝 - 신경망으로서의 로지스틱 회귀","name":"코세라) 신경망 및 딥러닝 - 신경망으로서의 로지스틱 회귀","description":"1. Binary Classification y가 0, 1일 때 사용하는 분류입니다. 로지스틱 회귀(Logistic regression)는 이진 분류를 위한 알고리즘 중 하나입니다. 2. Notation ","keywords":["neural network","구글 머신러닝 부트캠프"],"articleBody":"1. Binary Classification y가 0, 1일 때 사용하는 분류입니다. 로지스틱 회귀(Logistic regression)는 이진 분류를 위한 알고리즘 중 하나입니다. 2. Notation 수식들을 표기위한 Notation, 너무 중요합니다. 꼭 이해하고 사용할 수 있어야합니다. x(i)를 열로 쌓는 방식도 있다고 하는데 사용안한다고 하네요(그림에 보면 엑스로 표시). 행으로 쌓는 방식이 계산에 더 유리하다고 합니다. 3. Logistic Regression 로지스틱 회귀 알고리즘을 사용한다면, 딥러닝은 학습을 통해 파라미터 w, b를 구하는 과정입니다. 오른쪽에 보시면 쎄타(theta)로 w, b를 한번에 표현하는 방식이 있는데 별로라고합니다. w, b로 나눠서 처리하는 것이 낫다고 하네요. 이유는 잘모르겠습니다. 별개의 변수이니 나눠서 처리하는 것이 이해하기 쉽겠죠? 4. Logistic Regression cost function 손실(loss)은 오차를 의미합니다. 손실은 y의 값과 y^의 차이입니다. 딥러닝은 학습을 통해 가장 손실이 낮은 파라미터를 찾아내는 과정이 되겠네요. loss 함수는 정해져있는 건 아닌것 같습니다. 보통 $$ L(\\hat{y}, y) = 1/2{(\\hat{y} - y)}^2 $$ 을 사용하는 것 같은데 logistic regression에는 로그로 표현한 $$ L(\\hat{y}, y) = -(y\\log{\\hat{y}} + (1-y)\\log({1-\\hat{y}})) $$ 을 사용합니다. 녹색 필기부분을 이해하는 것이 중요합니다. y = 0, 1일때를 나눠서 설명하고 있습니다. y = 1 일때, 우리는 y^을 1에 가깝게 만들기를 원하고 그것은 loss가 0에 가깝기를 원하는 것과 같습니다. x가 1일때 0이 되는 로그함수와 정확하게 일치하는 개념입니다. y = 0 일때는 반대로 생각하시면 됩니다. 비용(cost) 함수는 m개의 훈련세트에 대한 손실을 m으로 나눈것입니다. 손실의 평균을 의미하네요. 5. Gradient Descent 기울기 하강(Gradient Descent)라는 개념자체를 알고 시작하는 것이 좋습니다. 기하학적으로 볼때 훈련을 반복하면서 볼록한 지점(비용이 최소)인 지점으로 점점 하강하는 과정을 상상할 수 있습니다. 볼록한 형태라는 것은 이미 밝혀져 있는 사실이라고 하네요. 이것이 궁금하면 수학과로 ㄱㄱ 6. Derivatives 이건 알아서 공부하시길.. 7. More derivatives examples 이건 알아서 공부하시길.. 8. Computation Graph Computation graph는 mathematical expression을 directed (acyclic) graph로 표현한 것입니다. 각 node는 하나의 operation(혹은 function)을 나타냅니다. 수식(cost function)이 주어졌을때, 그래프로 표현할 수 있습니다. 굳이 그래프로 표현하는 이유는 쉽게 이해하기 위함이라고 생각합니다. 각 계산과정이 레이어로 표현되어 과정을 한눈에 볼수있으니까요. 9. Derivatives with a Computation Graph 계산 그래프에서 앞으로 계산해나가는 것(전파)이 비용을 계산하는 것이라면, 뒤로 계산해 나가는것(역전파)는 각 수식에서 도함수를 구하는 과정입니다. 10. Logistic Regression Gradient descent 빨간선을 오른쪽에서 왼쪽으로 보면 연쇄법칙을 이용해 각 도함수를 구해나가는 과정을 볼 수 있습니다. 로그함수의 미분과 시그모이드 함수의 미분을 알아야 합니다. 미분표 참고하면 됩니다. 11. Gradient descent on m examples m 개의 훈련세트에 대해 cost를 구하고 반대로 도함수를 구하면서 w, b를 구하고, 마지막에 평균으로 나눠 최적의 w, b를 찾는 코드를 확인할 수 있습니다. 이해안가는 부분이 몇개 있긴한데, 과제를 풀면서 해결하면 될 것 같습니다. 예상은 했지만 생각보다 만만하지 않은 과정이네요.\n꽤나 열심히 해야되겠습니다.\n","wordCount":"1251","inLanguage":"ko","datePublished":"2024-07-14T17:05:02+09:00","dateModified":"2024-07-14T17:05:02+09:00","author":{"@type":"Person","name":"Jaejin Jang"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jaejin0me.github.io/post/20240713/"},"publisher":{"@type":"Organization","name":"Jaejin's blog","logo":{"@type":"ImageObject","url":"https://jaejin0me.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jaejin0me.github.io/ accesskey=h title="Jaejin's blog (Alt + H)">Jaejin's blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://jaejin0me.github.io/ title=Home><span>Home</span></a></li><li><a href=https://jaejin0me.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://jaejin0me.github.io/post/ title=Posts><span>Posts</span></a></li><li><a href=https://jaejin0me.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://jaejin0me.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://jaejin0me.github.io/about/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">코세라) 신경망 및 딥러닝 - 신경망으로서의 로지스틱 회귀</h1><div class=post-meta><span title='2024-07-14 17:05:02 +0900 +0900'>7월 14, 2024</span>&nbsp;·&nbsp;Jaejin Jang</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>목차</span></summary><div class=inner><ul><li><a href=#1-binary-classification aria-label="1. Binary Classification">1. Binary Classification</a></li><li><a href=#2-notation aria-label="2. Notation">2. Notation</a></li><li><a href=#3-logistic-regression aria-label="3. Logistic Regression">3. Logistic Regression</a></li><li><a href=#4-logistic-regression-cost-function aria-label="4. Logistic Regression cost function">4. Logistic Regression cost function</a></li><li><a href=#5-gradient-descent aria-label="5. Gradient Descent">5. Gradient Descent</a></li><li><a href=#6-derivatives aria-label="6. Derivatives">6. Derivatives</a></li><li><a href=#7-more-derivatives-examples aria-label="7. More derivatives examples">7. More derivatives examples</a></li><li><a href=#8-computation-graph aria-label="8. Computation Graph">8. Computation Graph</a></li><li><a href=#9-derivatives-with-a-computation-graph aria-label="9. Derivatives with a Computation Graph">9. Derivatives with a Computation Graph</a></li><li><a href=#10-logistic-regression-gradient-descent aria-label="10. Logistic Regression Gradient descent">10. Logistic Regression Gradient descent</a></li><li><a href=#11-gradient-descent-on-m-examples aria-label="11. Gradient descent on m examples">11. Gradient descent on m examples</a></li></ul></div></details></div><div class=post-content><h3 id=1-binary-classification>1. Binary Classification<a hidden class=anchor aria-hidden=true href=#1-binary-classification>#</a></h3><p><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_1.png alt></p><ul><li>y가 0, 1일 때 사용하는 분류입니다.</li><li>로지스틱 회귀(Logistic regression)는 이진 분류를 위한 알고리즘 중 하나입니다.</li></ul><h3 id=2-notation>2. Notation<a hidden class=anchor aria-hidden=true href=#2-notation>#</a></h3><p><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_2.png alt></p><ul><li>수식들을 표기위한 Notation, 너무 중요합니다. 꼭 이해하고 사용할 수 있어야합니다.</li><li>x(i)를 열로 쌓는 방식도 있다고 하는데 사용안한다고 하네요(그림에 보면 엑스로 표시). 행으로 쌓는 방식이 계산에 더 유리하다고 합니다.</li></ul><h3 id=3-logistic-regression>3. Logistic Regression<a hidden class=anchor aria-hidden=true href=#3-logistic-regression>#</a></h3><p><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_3.png alt></p><ul><li>로지스틱 회귀 알고리즘을 사용한다면, 딥러닝은 학습을 통해 파라미터 w, b를 구하는 과정입니다.</li><li>오른쪽에 보시면 쎄타(theta)로 w, b를 한번에 표현하는 방식이 있는데 별로라고합니다. w, b로 나눠서 처리하는 것이 낫다고 하네요. 이유는 잘모르겠습니다. 별개의 변수이니 나눠서 처리하는 것이 이해하기 쉽겠죠?</li></ul><h3 id=4-logistic-regression-cost-function>4. Logistic Regression cost function<a hidden class=anchor aria-hidden=true href=#4-logistic-regression-cost-function>#</a></h3><p><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_4.png alt></p><ul><li>손실(loss)은 오차를 의미합니다. 손실은 y의 값과 y^의 차이입니다. 딥러닝은 학습을 통해 가장 손실이 낮은 파라미터를 찾아내는 과정이 되겠네요.</li><li>loss 함수는 정해져있는 건 아닌것 같습니다. 보통 $$ L(\hat{y}, y) = 1/2{(\hat{y} - y)}^2 $$ 을 사용하는 것 같은데 logistic regression에는 로그로 표현한 $$ L(\hat{y}, y) = -(y\log{\hat{y}} + (1-y)\log({1-\hat{y}})) $$ 을 사용합니다.</li><li>녹색 필기부분을 이해하는 것이 중요합니다. y = 0, 1일때를 나눠서 설명하고 있습니다.</li><li>y = 1 일때, 우리는 y^을 1에 가깝게 만들기를 원하고 그것은 loss가 0에 가깝기를 원하는 것과 같습니다. x가 1일때 0이 되는 로그함수와 정확하게 일치하는 개념입니다.</li><li>y = 0 일때는 반대로 생각하시면 됩니다.</li><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_5.png alt></li><li>비용(cost) 함수는 m개의 훈련세트에 대한 손실을 m으로 나눈것입니다. 손실의 평균을 의미하네요.</li></ul><h3 id=5-gradient-descent>5. Gradient Descent<a hidden class=anchor aria-hidden=true href=#5-gradient-descent>#</a></h3><ul><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_8.png alt></li><li>기울기 하강(Gradient Descent)라는 개념자체를 알고 시작하는 것이 좋습니다.</li><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_6.png alt></li><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_7.png alt></li><li>기하학적으로 볼때 훈련을 반복하면서 볼록한 지점(비용이 최소)인 지점으로 점점 하강하는 과정을 상상할 수 있습니다.<ul><li>볼록한 형태라는 것은 이미 밝혀져 있는 사실이라고 하네요. 이것이 궁금하면 수학과로 ㄱㄱ</li></ul></li></ul><h3 id=6-derivatives>6. Derivatives<a hidden class=anchor aria-hidden=true href=#6-derivatives>#</a></h3><ul><li>이건 알아서 공부하시길..</li></ul><h3 id=7-more-derivatives-examples>7. More derivatives examples<a hidden class=anchor aria-hidden=true href=#7-more-derivatives-examples>#</a></h3><ul><li>이건 알아서 공부하시길..</li></ul><h3 id=8-computation-graph>8. Computation Graph<a hidden class=anchor aria-hidden=true href=#8-computation-graph>#</a></h3><ul><li>Computation graph는 mathematical expression을 directed (acyclic) graph로 표현한 것입니다. 각 node는 하나의 operation(혹은 function)을 나타냅니다.</li><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_9.png alt></li><li>수식(cost function)이 주어졌을때, 그래프로 표현할 수 있습니다. 굳이 그래프로 표현하는 이유는 쉽게 이해하기 위함이라고 생각합니다. 각 계산과정이 레이어로 표현되어 과정을 한눈에 볼수있으니까요.</li></ul><h3 id=9-derivatives-with-a-computation-graph>9. Derivatives with a Computation Graph<a hidden class=anchor aria-hidden=true href=#9-derivatives-with-a-computation-graph>#</a></h3><ul><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_10.png alt></li><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_11.png alt></li><li>계산 그래프에서 앞으로 계산해나가는 것(전파)이 비용을 계산하는 것이라면, 뒤로 계산해 나가는것(역전파)는 각 수식에서 도함수를 구하는 과정입니다.</li></ul><h3 id=10-logistic-regression-gradient-descent>10. Logistic Regression Gradient descent<a hidden class=anchor aria-hidden=true href=#10-logistic-regression-gradient-descent>#</a></h3><ul><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_12.png alt></li><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_13.png alt></li><li>빨간선을 오른쪽에서 왼쪽으로 보면 연쇄법칙을 이용해 각 도함수를 구해나가는 과정을 볼 수 있습니다.</li><li>로그함수의 미분과 시그모이드 함수의 미분을 알아야 합니다. 미분표 참고하면 됩니다.</li></ul><h3 id=11-gradient-descent-on-m-examples>11. Gradient descent on m examples<a hidden class=anchor aria-hidden=true href=#11-gradient-descent-on-m-examples>#</a></h3><ul><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_14.png alt></li><li><img loading=lazy src=/%eb%a1%9c%ec%a7%80%ec%8a%a4%ed%8b%b1%ed%9a%8c%ea%b7%80_15.png alt></li><li>m 개의 훈련세트에 대해 cost를 구하고 반대로 도함수를 구하면서 w, b를 구하고, 마지막에 평균으로 나눠 최적의 w, b를 찾는 코드를 확인할 수 있습니다.</li><li>이해안가는 부분이 몇개 있긴한데, 과제를 풀면서 해결하면 될 것 같습니다.</li></ul><hr><p>예상은 했지만 생각보다 만만하지 않은 과정이네요.<br>꽤나 열심히 해야되겠습니다.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://jaejin0me.github.io/tags/neural-network/>Neural Network</a></li><li><a href=https://jaejin0me.github.io/tags/%EA%B5%AC%EA%B8%80-%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%EB%B6%80%ED%8A%B8%EC%BA%A0%ED%94%84/>구글 머신러닝 부트캠프</a></li></ul></footer><script src=https://utteranc.es/client.js repo=jaejin0me/blog_comment issue-term=title label=comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>Jaejin Jang</span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>